###一、KNN Classification

####Introduction

KNN是一个鲁棒性比较好的分类器，主要原理归结于通过对离新数据点最近的K个数据点的类别进行投票法，选出票数最多的类别作为该数据点的最终预测。KNN的分类性能常作为诸如SVM这样复杂分类器的参照基准。KNN虽然简单，但是其分类性能远强于其他的分类器。

KNN是非参的、基于实例的有监督学习算法：

+ Non-parametric：非参指的是不对隐藏在数据中的分布函数做假设，从而降低了训练所得之模型不能很好的拟合数据的风险。举个例子就是，假设数据并不符合高斯分布，但采用了遵从高斯分布的学习算法所得到的模型，其对数据的预测能力就会非常差。
+ Instance-based：基于实例指的是算法并没有显式地从数据中拟合出一个模型，算法只是记住了所有的训练数据并将之作为预测的信息，这就相当于是从数据库管理系统中去查询数据一样，数据都存储在数据库中。既然是这样子，那么KNN的训练过程就非常占用内存空间，因为需要将所有的训练数据都存储起来，而验证过程就很消耗计算资源了，因为需要遍历整个训练数据集里的数据去计算出和所要预测的数据的距离，然后进一步通过投票法来预测数据最终归于哪一类。因此，KNN训练过程消耗存储，验证过程消耗计算资源。

#### KNN数学原理

KNN分类算法的核心在于投票法，主要开销用在对距离的计算上。**优化$KNN$算法就是如何优化距离算法**。最快的方法是进行矩阵计算，具体是将整体训练数据点作为一个矩阵，一个测试数据作为向量，通过矩阵与向量的点积得到测试数据与训练数据之间的距离。

相似距离一般采用欧式距离，也有曼哈顿距离、切比雪夫距离和汉明距离，以下为欧式距离，令新数据为$X=(x_{1},...,x_{m})$，训练数据为$X^{'}=(x^{'}_{1},...,x^{'}_{m})$：
$$
d(X,X^{'})=\sqrt{ (x_{1}-x^{'}_{1})^{2} +...+(x_{m}-x^{'}_{m})^{2}}
$$
KNN分类算法主要是重复以下两个步骤：

+ 遍历整个训练集，并计算出新数据$X$和整个训练集中的数据的距离$d$，并将训练数据中和新数据最近的K个点构成集合$A$， 构成的这个$A$集合，就是一个类别。K的选择通常选择奇数，这样可以防止投票法遇到“平局”的情况。

+ 得到距离后，再根据条件概率计算出新数据点属于某个类别的概率，也就是对新数据与集合$A$中每个元素算出平均概率
  $$
  P(y=j|X=x)=\frac {1}{K}\sum_{i \in A}I(y^{i}=j)
  $$
  其中，函数$I$定义为：
  $$
  I(x)=\left\{\begin{matrix}
   1 \ \ \ \ \ x=true\\ 
   0 \ \ \ \ \ x=false
  \end{matrix}\right.
  $$
  最终选出概率最大的作为该新数据的类别。

#### K值的选择

$k$值的选择会对$KNN$的结果产生重大影响。如果选择较小的$k$值，就相当于用较小的邻域中的训练实例进行预测，意味着整体模型变复杂，容易发生过拟合。如果选择较大的$k$值，就相当于用较大邻域中的训练实例进行预测，意味着整体的模型变得简单。如果$k$设置为N，那么无论输入是什么，都将简单地预测它属于在训练实例中最多的类。这时模型过于简单，完全忽略训练实例中的大量有用信息，是不可取的。

#### KNN算法的短处

KNN分类算法在验证阶段的开销是巨大的，而且还慢，此外，如果数据类别是不平衡的，KNN分类算法就失效了，因为它会把类别数最多的那个类别作为预测结果。如果数据是高维的，那么最近点和最远点之间的距离差别不大。

因此KNN算法可以作如下改进：

+ 在处理不平衡数据时，可以采用**带权重的投票法**。
+ 不同的具体场景采用不同的距离测度作为相似度
+ 对数据进行规范化处理
+ 数据将维处理

